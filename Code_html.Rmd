---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---
```{r, echo=FALSE}
library(jsonlite)
library(stringr)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(nnet)
library(glmnet)
library(randomForest)
library(adabag)
```
## Cleaning Data ##

**Loading data sets**

```{r, echo=TRUE}
user_data <- load("yelp_user_small.Rda")
review_data <- load("yelp_review_small.Rda")
business_data <- stream_in(file("yelp_academic_dataset_business.json")) 
full_user_data <- stream_in(file("yelp_academic_dataset_user.json"))
```

**Checking for duplicate entries**

```{r}
print(sum(duplicated(business_data$business_id)))
print(revdup <- sum(duplicated(review_data_small$business_id)))
print(usdup <- sum(duplicated(review_data_small$user_id)))
```

business_data: 0 business have been included twice
review_data: \( \frac{1,261,472}{1,398,056} \) have been reviewed more than once. \( \frac{686,446}{1,398,056} \) users have left more than 1 review

```{r}
sum(is.na(user_data_small))
sum(is.na(review_data_small))
```
*there are no missing values in user_data or review_data


user_data_small$elite has only 18,178 entries for "elite" column out of 397,579, although not coded as "na" they can be viewed as missing values and can be dropped
```{r}
# Write code here how you got to the bottom code
user_data_small$elite <- NULL
```

user_data_small$friends can be transformed to display number of friends although the attributes of a user's friends likely has predictive power it will be difficult to model

coding all "none" values as 0, and adding a friend count
```{r}
user_data_small$friends <- ifelse(user_data_small$friends=="None",0,str_count(user_data_small$friends, ",") + 1)
```

Selecting relevant predictors from business_data dataset
```{r}
business_data <- unnest(business_data)

isna_business_data <- round(colSums(is.na(business_data))/nrow(business_data) * 100,2)

# columns with missing values will not be included in data set

business_data_new <- subset(business_data, select = c("state", "stars", "review_count", "is_open", "business_id", )
```

merging review data set with user data set and business data set

```{r}
merged_df <- merge(merge(review_data_small, user_data_small, by = "user_id"), business_data_new, by = "business_id")

# making "state" into a factor variable
merged_df$state <- factor(merged_df$state)
```

counting the number of zero values in each column

```{r}
# Count the number of zeros in each column
num_zeros_per_column <- round((colSums(merged_df == 0)/nrow(merged_df))*100, 2)

x_values = names(num_zeros_per_column)
y_values = unname(num_zeros_per_column)

col_sums <- data.frame(variable = x_values, proportion = y_values)
col_sums <- col_sums[col_sums$proportion != 0,]


# creating a plot

zeros_plot <- ggplot(data = col_sums, mapping = aes(x=reorder(variable,-proportion), y = proportion)) + geom_bar(stat = "identity", fill = "skyblue") + geom_hline(yintercept = 30, linetype="dashed", color="red", linewidth =1) + 
  ggtitle("Percentage of 0 values per column") + 
  labs(x="Variable name", y = "Percentage (%)") +
  theme(axis.text.x=element_text(angle=45, hjust=1), 
        plot.title    = element_text(family = "mono", hjust =0.5),
        axis.title.x = element_text(family="mono"),
        axis.title.y = element_text(family="mono")
        ) 

zeros_plot
```
Removing columns with over 30% 0 values as they are likely not predictive and
due to limited computational power

```{r}
# Identify columns where the percentage of 0s is greater than 30%
columns_to_remove <- names(num_zeros_per_column[num_zeros_per_column > 30])

# Remove identified columns from the data frame
merged_df <- merged_df[, !names(merged_df) %in% columns_to_remove]
```




Removing irrelevant variables
dropping "city" because certain cities will not be common to both train and test data
and will generate an error

```{r}
clean_df <- merged_df %>% select(-c("business_id","date", "user_id", "review_id")) %>%
  rename(review_stars = stars.x, review_count_user = review_count.x, useful_user = useful.y, funny = funny.y, cool = cool.y , user_stars = average_stars, review_count_business = review_count.y, business_stars = stars.y)

## Removing states with less than 10 reviews
states_count <- table(clean_df$state)
states_to_remove <- names(states_count[states_count < 10])
clean_df <- clean_df[!(clean_df$state %in% states_to_remove), ]

# making "state" a factor variable
# making "review_stars" into a factor variable, due to classification problem

clean_df$state <- factor(clean_df$state)
clean_df$review_stars <- factor(clean_df$review_stars)


# dropping irrelevant variables

clean_df <- clean_df %>% select(-c("name", "text", "yelping_since"))
```

## Test vs Train ##

splitting into test and train

```{r}
set.seed(1)
train_split <- sample(1:nrow(clean_df), 3*nrow(clean_df)/4)


train <- clean_df[train_split,] 
test <- clean_df[(-train_split),] 

trainX <- subset(train,select = -review_stars)
trainY <- subset(train, select = review_stars)
testX <- subset(test, select = -review_stars)
testY <- subset(test, select = review_stars)
```

## Random Choice model ##

```{r}
randY <- sample(1:5, length(testY$review_stars), replace = TRUE)
```
#####
random choice MSE

```{r}
rand_MSE<-mean((randY-testY$review_stars)^2)
rand_MSE
```
#############
random classification accuracy

```{r}
randcount <- sum(randY==testY)
randcount

randclass_acc <- randcount/length(testY$review_stars)

randclass_acc
```


## Multinomial Logit Model ##


fitting multinomial logit model due to discreet outcome variables

```{r}
ml_model <- multinom(review_stars ~., data=train)
```
multinom predictions

```{r}
mlmod_predict <- predict(ml_model, newdata = testX)
```

multinom MSE

```{r}
ml_MSE<-mean((as.numeric(mlmod_predict)-testY$review_stars)^2)

ml_MSE
```

classification accuracy

```{r}
mlcount <- sum(as.numeric(mlmod_predict)==testY)
mlcount

mlclass_acc <- mlcount/length(testY$review_stars)

mlclass_acc
```

## OLS ##

fitting a linear model

```{r}
linmod <- lm(review_stars ~., data=train)
```

LM predictions

```{r}
linmod_predict <- predict(linmod, newdata = testX)
```

LM MSE calculations

```{r}
lm_MSE<-mean((linmod_predict-testY$review_stars)^2)

lm_MSE
```

## Shrinkage methods ##

### Ridge
**Ridge** 

```{r}
ridge.mod<-glmnet(trainX, trainY$review_stars, alpha = 0, lambda = 3, thresh = 1e-12)
coef(ridge.mod) #coefficients

# Cross - validation


cv.out = cv.glmnet(as.matrix(trainX), as.matrix(trainY$review_stars), alpha = 0, nfolds = 10) 
bestlam = cv.out$lambda.min  


plot(cv.out) + 
  title("")


```

Prediction of Ridge

```{r}
ridge.mod<-glmnet(trainX, trainY$review_stars, alpha = 0, lambda = bestlam, thresh = 1e-12)
coef(ridge.mod) #coefficients
ridge_pred = predict(ridge.mod, s = bestlam, newx = as.matrix(testX)) # Use best lambda to predict test data
mean((ridge_pred - testY$review_stars)^2) # Calculate test MSE
```
### Lasso

Prediction of Lasso


```{r}
cv.out1 = cv.glmnet(as.matrix(trainX), as.matrix(trainY$review_stars), alpha = 1, nfolds = 10) 
bestlam1 = cv.out1$lambda.min  


plot(cv.out1) + 
  title("")
```



Prediction

```{r}
lasso.mod<-glmnet(trainX, trainY$review_stars, alpha = 1, lambda = bestlam1, thresh = 1e-12)
lasso_pred = predict(lasso.mod, s = bestlam1, newx = as.matrix(testX)) # Use best lambda to predict test data
mean((lasso_pred - testY$review_stars)^2) # Calculate test MSE
```

### Tree Based Methods ### 

## Random Forest ##

```{r}
rf <- randomForest(review_stars~., data=train, proximity=FALSE, ntree =100)
rf_predict <- predict(rf, newdata = testX)

# RF classification accuracy

rfcount <- sum(as.numeric(rf_predict)==testY)

rfclass_acc <- rfcount/length(testY$review_stars)

rfclass_acc

plot(rf)

```

### Boosting ##

```{r}
# train a model using our training data
model_adaboost <- boosting(review_stars~., data=train, boos=TRUE, mfinal=50)
summary(model_adaboost)

ab_predict <- predict(model_adaboost, newdata = testX)

# RF classification accuracy

abcount <- sum(as.numeric(ab_predict$class)==testY$review_stars)

abclass_acc <- abcount/length(testY$review_stars)

abclass_acc


```


